<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>12. Deep Metric Learning &mdash; fong alpha documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="数理与算法" href="../mathematicsAlgorithm/index.html" />
    <link rel="prev" title="11. pytorch：no_grad()" href="11_nograd.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> fong
            <img src="../_static/logo.jpg" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                alpha
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">目录</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../cpp/index.html">C/C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python/index.html">Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../linux/index.html">Linux/Shell</a></li>
<li class="toctree-l1"><a class="reference internal" href="../git/index.html">Git</a></li>
<li class="toctree-l1"><a class="reference internal" href="../machineLearning/index.html">机器学习</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">深度学习</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01_dataParallel.html">1. pytorch：多GPU模式</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_activationFunction.html">2. 激活函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_batchnorm.html">3. Batch Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="04_overfit.html">4. 过拟合</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_modelSave.html">5. pytorch：模型保存与读取</a></li>
<li class="toctree-l2"><a class="reference internal" href="06_cuda.html">6. pytorch: cuda</a></li>
<li class="toctree-l2"><a class="reference internal" href="07_backprop.html">7. 反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="08_optimizer.html">8. 优化算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="09_addModule.html">9. pytorch：Module</a></li>
<li class="toctree-l2"><a class="reference internal" href="10_receptivaField.html">10. 特征图与感受野</a></li>
<li class="toctree-l2"><a class="reference internal" href="11_nograd.html">11. pytorch：no_grad()</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">12. Deep Metric Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">12.1. 损失函数</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#softmax-loss">Softmax Loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="#center-loss">Center Loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="#large-margin-softmax-loss">Large Margin Softmax Loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sphereface-loss">SphereFace Loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cosface-loss">CosFace Loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="#arcface-loss">ArcFace Loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="#contrastive-loss">Contrastive Loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="#triplet-loss">Triplet Loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="#margin-loss">Margin Loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="#tuplet-margin-loss">Tuplet Margin Loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="#angular-loss">Angular Loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="#n-pair-loss">N-pair Loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="#lifted-structure-loss">Lifted Structure Loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="#nca-loss">NCA Loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="#proxy-nca-loss">Proxy NCA Loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="#proxy-anchor-loss">Proxy Anchor Loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="#softtriple-loss">SoftTriple Loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multi-similarity-loss">Multi-Similarity loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="#normalized-temperature-scaled-cross-entropy-loss">Normalized Temperature-scaled Cross Entropy Loss</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id2">12.2. 样本对挖掘</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#packaged-triplets">Packaged Triplets</a></li>
<li class="toctree-l4"><a class="reference internal" href="#triplet-margin-miner">Triplet-Margin Miner</a></li>
<li class="toctree-l4"><a class="reference internal" href="#angular-miner">Angular Miner</a></li>
<li class="toctree-l4"><a class="reference internal" href="#batch-hard-miner">Batch-Hard Miner</a></li>
<li class="toctree-l4"><a class="reference internal" href="#distance-weighted-miner">Distance-Weighted Miner</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hdc-miner">HDC Miner</a></li>
<li class="toctree-l4"><a class="reference internal" href="#maximum-loss-miner">Maximum-Loss Miner</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multi-similarity-miner">Multi-Similarity Miner</a></li>
<li class="toctree-l4"><a class="reference internal" href="#pair-margin-miner">Pair-Margin Miner</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id4">12.3. 参考资料</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mathematicsAlgorithm/index.html">数理与算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../computerNetwork/index.html">计算机网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../link/index.html">资源链接</a></li>
<li class="toctree-l1"><a class="reference internal" href="../softwares/index.html">实用软件</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tech/index.html">Tech</a></li>
<li class="toctree-l1"><a class="reference internal" href="../else/index.html">其他</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">fong</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="index.html">深度学习</a> &raquo;</li>
      <li><span class="section-number">12. </span>Deep Metric Learning</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/deepLearning/12_dml.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="deep-metric-learning">
<h1><span class="section-number">12. </span>Deep Metric Learning<a class="headerlink" href="#deep-metric-learning" title="Permalink to this heading"></a></h1>
<p>介绍一些 Deep Metric Learning （深度度量学习）的损失函数和训练样本对挖掘方法。</p>
<section id="id1">
<h2><span class="section-number">12.1. </span>损失函数<a class="headerlink" href="#id1" title="Permalink to this heading"></a></h2>
<p>以下损失函数中的 <span class="math notranslate nohighlight">\(x\)</span> 表示 embedding。</p>
<section id="softmax-loss">
<h3>Softmax Loss<a class="headerlink" href="#softmax-loss" title="Permalink to this heading"></a></h3>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_i) = - \log \left( \frac{e^{w^{\top}_{y_i} x_i + b_{y_i}}}{\sum_j e^{w^{\top}_j x_i + b_j}} \right)\]</div>
</section>
<section id="center-loss">
<h3>Center Loss<a class="headerlink" href="#center-loss" title="Permalink to this heading"></a></h3>
<p><a class="reference external" href="https://kpzhang93.github.io/papers/eccv2016.pdf">A Discriminative Feature Learning Approach for Deep Face Recognition</a></p>
<p>减小类内差异，每个类别在特征空间分别维护一个类中心。</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathcal{L}(x_i) \ &amp;=\  \frac{1}{2} \| x_i - c_{y_i} \|_2^2 \\
\Delta c_j \ &amp;=\  \frac{\sum_{i=1}^{m} \delta(y_i=j) \cdot (c_j - x_i)}{1 + \sum_{i=1}^{m} \delta(y_i=j)}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(m\)</span> 是一个 batch 的大小。</p>
</section>
<section id="large-margin-softmax-loss">
<h3>Large Margin Softmax Loss<a class="headerlink" href="#large-margin-softmax-loss" title="Permalink to this heading"></a></h3>
<p><a class="reference external" href="https://arxiv.org/pdf/1612.02295.pdf">Large-Margin Softmax Loss for Convolutional Neural Networks</a></p>
<p>减小类内差异，增大类间差异。</p>
<div class="math notranslate nohighlight">
\[w_j^{\top} x_i = \| w_j \| \| x_i \| \cos(\theta_j)\]</div>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_i) = - \log \left( \frac{e^{\| w_{y_i} \| \| x_i \| \psi(\theta_{y_i}) }}{e^{\| w_{y_i} \| \| x_i \| \psi(\theta_{y_i}) } + \sum_{j \neq y_i} e^{\| w_j \| \| x_i \| \cos(\theta_j) }} \right)\]</div>
<div class="math notranslate nohighlight">
$$
\psi(\theta) =
\begin{cases}
   \cos (m \theta) &amp; &amp; 0 \leq \theta \leq \frac{\pi}{m} \\
   D(\theta) &amp; &amp;  \frac{\pi}{m} &lt; \theta \leq \pi
\end{cases}
$$</div><p><span class="math notranslate nohighlight">\(m\)</span> 表示 margin，<span class="math notranslate nohighlight">\(D(\theta)\)</span> 是一个单调减函数，且 <span class="math notranslate nohighlight">\(D(\frac{\pi}{m})=\cos(\frac{\pi}{m})\)</span> 。</p>
</section>
<section id="sphereface-loss">
<h3>SphereFace Loss<a class="headerlink" href="#sphereface-loss" title="Permalink to this heading"></a></h3>
<p><a class="reference external" href="https://arxiv.org/pdf/1704.08063.pdf">SphereFace: Deep Hypersphere Embedding for Face Recognition</a></p>
<p>在 Large Margin Softmax Loss 的基础上，令 <span class="math notranslate nohighlight">\(\| w \| = 1\)</span> 。</p>
<div class="math notranslate nohighlight">
\[w_j^{\top} x_i = \| w_j \| \| x_i \| \cos(\theta_j) = \| x_i \| \cos(\theta_j)\]</div>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_i) = - \log \left( \frac{e^{\| x_i \| \cos(m \theta_{y_i}) }}{e^{\| x_i \| \cos(m \theta_{y_i}) } + \sum_{j \neq y_i} e^{\| x_i \| \cos(\theta_j) }} \right)\]</div>
</section>
<section id="cosface-loss">
<h3>CosFace Loss<a class="headerlink" href="#cosface-loss" title="Permalink to this heading"></a></h3>
<p><a class="reference external" href="https://arxiv.org/pdf/1801.09414.pdf">CosFace: Large Margin Cosine Loss for Deep Face Recognition</a></p>
<p>在余弦空间中最大化分类界限。</p>
<div class="math notranslate nohighlight">
\[w_j^{\top} x_i = \| w_j \| \| x_i \| \cos(\theta_j) = \| x_i \| \cos(\theta_j) = s \cos(\theta_j)\]</div>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_i) = - \log \left( \frac{e^{s(\cos(\theta_{y_i}) - m)}}{e^{s(\cos(\theta_{y_i}) - m)} + \sum_{j \neq y_i} e^{s \cos(\theta_j)}} \right)\]</div>
<p><span class="math notranslate nohighlight">\(m\)</span> 表示 margin，<span class="math notranslate nohighlight">\(s\)</span> 表示超球面的半径。</p>
</section>
<section id="arcface-loss">
<h3>ArcFace Loss<a class="headerlink" href="#arcface-loss" title="Permalink to this heading"></a></h3>
<p><a class="reference external" href="https://arxiv.org/pdf/1801.07698.pdf">ArcFace: Additive Angular Margin Loss for Deep Face Recognition</a></p>
<p>在角度空间中最大化分类界限。</p>
<div class="math notranslate nohighlight">
\[w_j^{\top} x_i = \| w_j \| \| x_i \| \cos(\theta_j) = \| x_i \| \cos(\theta_j) = s \cos(\theta_j)\]</div>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_i) = - \log \left( \frac{e^{s(\cos(\theta_{y_i} + m))}}{e^{s(\cos(\theta_{y_i} + m))} + \sum_{j \neq y_i} e^{s \cos(\theta_j)}} \right)\]</div>
<p><span class="math notranslate nohighlight">\(m\)</span> 表示 margin，<span class="math notranslate nohighlight">\(s\)</span> 表示超球面的半径。</p>
</section>
<section id="contrastive-loss">
<h3>Contrastive Loss<a class="headerlink" href="#contrastive-loss" title="Permalink to this heading"></a></h3>
<p><a class="reference external" href="http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf">Dimensionality Reduction by Learning an Invariant Mapping</a></p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_i, x_j) = \mathbf{1} (y_i = y_j) \| x_i - x_j \|_2^2 + \mathbf{1} (y_i \neq y_j) max(0, m - \| x_i - x_j \|_2)^2\]</div>
</section>
<section id="triplet-loss">
<h3>Triplet Loss<a class="headerlink" href="#triplet-loss" title="Permalink to this heading"></a></h3>
<p><a class="reference external" href="https://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf">Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_a, x_p, x_n) = max(0, m + \| x_a - x_p \|_2^2 - \| x_a - x_n \|_2^2)\]</div>
</section>
<section id="margin-loss">
<h3>Margin Loss<a class="headerlink" href="#margin-loss" title="Permalink to this heading"></a></h3>
<p><a class="reference external" href="https://arxiv.org/pdf/1706.07567.pdf">Sampling Matters in Deep Embedding Learning</a></p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_i, x_j) = max(0, \alpha + y_{ij} (D_{i,j} - \beta))\]</div>
<p><span class="math notranslate nohighlight">\(y_{ij} \in \{ -1, 1 \}\)</span>，<span class="math notranslate nohighlight">\(D_{ij}\)</span> 表示距离，<span class="math notranslate nohighlight">\(\beta\)</span> 是可学习的参数。</p>
</section>
<section id="tuplet-margin-loss">
<h3>Tuplet Margin Loss<a class="headerlink" href="#tuplet-margin-loss" title="Permalink to this heading"></a></h3>
<p><a class="reference external" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Deep_Metric_Learning_With_Tuplet_Margin_Loss_ICCV_2019_paper.pdf">Deep Metric Learning with Tuplet Margin Loss</a></p>
<p>每个 batch 包含 <span class="math notranslate nohighlight">\(k\)</span> 个类别，每个类别 <span class="math notranslate nohighlight">\(n\)</span> 个样本，从其他的 <span class="math notranslate nohighlight">\(k-1\)</span> 个类别中随机选取一个样本作为负例，可以组成 <span class="math notranslate nohighlight">\(kn(n-1)\)</span> 个三元组。</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_a, x_p) = \log \left( 1 + \sum_{i=1}^{k-1} e^{s \left( \cos(\theta_{an_i}) - \cos(\theta_{ap} - \beta) \right)} \right)\]</div>
<p><span class="math notranslate nohighlight">\(s\)</span> 是一个缩放因子。</p>
</section>
<section id="angular-loss">
<h3>Angular Loss<a class="headerlink" href="#angular-loss" title="Permalink to this heading"></a></h3>
<p><a class="reference external" href="https://arxiv.org/pdf/1708.01682.pdf">Deep Metric Learning with Angular Loss</a></p>
<p>以三元组的角度为优化目标，最小化负样本对应的 <span class="math notranslate nohighlight">\(\angle n\)</span> 。相比于 Triplet Loss 的优势：角度具有缩放不变性；角度的计算过程可以利用到三条边；角度的阈值比 Triplet Loss 的 margin 具有更明确的物理意义。</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\mathcal{B}) = \frac{1}{N} \sum_{x_a \in \mathcal{B}} \log \left( 1 + \sum_{x_n \in \mathcal{B}} e^{f_{a,p,n}} \right)\]</div>
<div class="math notranslate nohighlight">
\[f_{a,p,n} = 4 \tan^2 \alpha (x_a + x_p)^{\top} x_n - 2 (1 + \tan^2 \alpha) x_a^{\top} x_p\]</div>
<p><span class="math notranslate nohighlight">\(\mathcal{B}\)</span> 表示一个 batch 的样本集合，<span class="math notranslate nohighlight">\(N\)</span> 是 batch 的大小，<span class="math notranslate nohighlight">\(\alpha\)</span> 是角度阈值。</p>
</section>
<section id="n-pair-loss">
<h3>N-pair Loss<a class="headerlink" href="#n-pair-loss" title="Permalink to this heading"></a></h3>
<p><a class="reference external" href="http://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf">Improved Deep Metric Learning with Multi-class N-pair Loss Objective</a></p>
<p>利用一个 batch 内的所有负例。</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_i, x_i^+) = \log \left( 1 + \sum_{j \neq i} e^{x_i^{\top} x_j^+ - x_i^{\top} x_i^+} \right)\]</div>
</section>
<section id="lifted-structure-loss">
<h3>Lifted Structure Loss<a class="headerlink" href="#lifted-structure-loss" title="Permalink to this heading"></a></h3>
<p><a class="reference external" href="https://arxiv.org/pdf/1511.06452.pdf">Deep Metric Learning via Lifted Structured Feature Embedding</a></p>
<p>利用一个 batch 内的所有正负样本对。</p>
<div class="math notranslate nohighlight">
\[\mathcal{L} = \frac{1}{2 | \mathcal{P} |} \sum_{(i,j) \in \mathcal{P}} max(0, \mathcal{L}_{i,j})^2\]</div>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{i,j} = max \left( \underset{(i,k) \in \mathcal{N}}{max}(\alpha - D_{i,k}), \underset{(j,l) \in \mathcal{N}}{max}(\alpha - D_{j,l}) \right) + D_{i,j}\]</div>
<p><span class="math notranslate nohighlight">\(\mathcal{P}\)</span> 表示正样本对，<span class="math notranslate nohighlight">\(\mathcal{N}\)</span> 表示负样本对，<span class="math notranslate nohighlight">\(D_{i,j}\)</span> 表示样本对的距离，<span class="math notranslate nohighlight">\(\alpha\)</span> 表示 margin。</p>
</section>
<section id="nca-loss">
<h3>NCA Loss<a class="headerlink" href="#nca-loss" title="Permalink to this heading"></a></h3>
<p><a class="reference external" href="https://www.cs.toronto.edu/~hinton/absps/nca.pdf">Neighbourhood Components Analysis</a></p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x, y, \mathcal{Z}) = - \log \left( \frac{e^{-d(x, y)}}{\sum_{z \in \mathcal{Z}} e^{-d(x,z)}} \right)\]</div>
<p><span class="math notranslate nohighlight">\(d\)</span> 是距离函数，<span class="math notranslate nohighlight">\(y\)</span> 是正例，<span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> 是负例集合。</p>
</section>
<section id="proxy-nca-loss">
<h3>Proxy NCA Loss<a class="headerlink" href="#proxy-nca-loss" title="Permalink to this heading"></a></h3>
<p><a class="reference external" href="https://arxiv.org/pdf/1703.07464.pdf">No Fuss Distance Metric Learning using Proxies</a></p>
<p>每一个类别都有一个可学习的 proxy，用来近似真实的数据点。<span class="math notranslate nohighlight">\(x\)</span> 对应的正例为本类别的 proxy <span class="math notranslate nohighlight">\(p(y)\)</span>，负例为所有其他类别的 proxy <span class="math notranslate nohighlight">\(p(\mathcal{Z})\)</span> 。</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x) = - \log \left( \frac{e^{-d(x, p(y))}}{\sum_{p(z) \in p(\mathcal{Z})} e^{-d(x,p(z))}} \right)\]</div>
</section>
<section id="proxy-anchor-loss">
<h3>Proxy Anchor Loss<a class="headerlink" href="#proxy-anchor-loss" title="Permalink to this heading"></a></h3>
<p><a class="reference external" href="https://arxiv.org/pdf/2003.13911.pdf">Proxy Anchor Loss for Deep Metric Learning</a></p>
<p>为每一个类别赋予了一个 proxy，将一个 batch 的样本和所有的 proxy 之间求距离，拉近每个类别的样本和该类别对应的 proxy 之间的距离，拉远与其他类别的 proxy 之间的距离。相比于 Proxy NCA Loss，更加充分地利用了 batch 的数据。</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\mathcal{X}) = \frac{1}{| \mathcal{P}^+ |} \sum_{p \in \mathcal{P}^+} \log \left( 1 + \sum_{x \in \mathcal{X}_p^+} e^{-\alpha (s(x,p) - \delta)} \right) + \frac{1}{| \mathcal{P} |} \sum_{p \in \mathcal{P}} \log \left( 1 + \sum_{x \in \mathcal{X}_p^-} e^{\alpha (s(x,p) + \delta)} \right)\]</div>
<p><span class="math notranslate nohighlight">\(\mathcal{X}\)</span> 表示一个 batch 内所有样本的 embedding 集合；<span class="math notranslate nohighlight">\(\mathcal{P}^+\)</span> 表示正例 proxy 的集合，也就是 batch 内的样本对应的 proxy 的集合；<span class="math notranslate nohighlight">\(\mathcal{P}\)</span> 表示所有 proxy 的集合，也就是所有类别对应的 proxy 的集合；<span class="math notranslate nohighlight">\(\mathcal{X}_p^+\)</span> 表示与 <span class="math notranslate nohighlight">\(p\)</span> 同一类别的 embedding 集合，<span class="math notranslate nohighlight">\(\mathcal{X}_p^- = \mathcal{X} - \mathcal{X}_p^+\)</span> ；<span class="math notranslate nohighlight">\(s\)</span> 表示余弦相似度。</p>
</section>
<section id="softtriple-loss">
<h3>SoftTriple Loss<a class="headerlink" href="#softtriple-loss" title="Permalink to this heading"></a></h3>
<p><a class="reference external" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Qian_SoftTriple_Loss_Deep_Metric_Learning_Without_Triplet_Sampling_ICCV_2019_paper.pdf">SoftTriple Loss: Deep Metric Learning Without Triplet Sampling</a></p>
<p>考虑到同类数据的多样性，为每类数据学习 <span class="math notranslate nohighlight">\(K\)</span> 个类中心；通过正则项自适应地合并相似的类中心。</p>
<div class="math notranslate nohighlight">
\[S_{i, c} = \sum_{k=1}^K \frac{e^{\frac{1}{\gamma}x_i^{\top}w_c^k}}{\sum_{t=1}^K e^{\frac{1}{\gamma}x_i^{\top}w_c^t}} x_i^{\top}w_c^k\]</div>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_i) = - \log \frac{e^{\lambda (S_{i, y_i} - \delta )}}{e^{\lambda (S_{i, y_i} - \delta )} + \sum_{j \neq y_i} e^{\lambda S_{i, j}}}\]</div>
</section>
<section id="multi-similarity-loss">
<h3>Multi-Similarity loss<a class="headerlink" href="#multi-similarity-loss" title="Permalink to this heading"></a></h3>
<p><a class="reference external" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Multi-Similarity_Loss_With_General_Pair_Weighting_for_Deep_Metric_Learning_CVPR_2019_paper.pdf">Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning</a></p>
<p>为给每一个样本对动态赋予一个权重，这个权重是体现在梯度上的。给样本赋权的核心在于判断样本的局部分布，即它们之间的相似性。局部样本之间的分布和相互关系并不仅仅取决于当前两个样本之间的距离或相似性，还取决于当前样本对与其周围样本对之间的关系。</p>
<div class="math notranslate nohighlight">
\[w_{ij}^- = \frac{1}{e^{\beta(\lambda - S_{ij})} + \sum_{k \in \mathcal{N}_i} e^{\beta(S_{ik} - S_{ij})}}\]</div>
<div class="math notranslate nohighlight">
\[w_{ij}^+ = \frac{1}{e^{-\alpha(\lambda - S_{ij})} + \sum_{k \in \mathcal{P}_i} e^{-\alpha(S_{ik} - S_{ij})}}\]</div>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_i) = \frac{1}{\alpha} \log \left( 1 + \sum_{k \in \mathcal{P}_i} e^{-\alpha (S_{ik} - \lambda)} \right) + \frac{1}{\beta} \log \left( 1 + \sum_{k \in \mathcal{N}_i} e^{\beta (S_{ik} - \lambda)} \right)\]</div>
<p><span class="math notranslate nohighlight">\(\mathcal{P}_i\)</span> 表示正样本集合，<span class="math notranslate nohighlight">\(\mathcal{N}_i\)</span> 表示负样本集合，<span class="math notranslate nohighlight">\(S_{ij}\)</span> 表示样本对的相似度。</p>
</section>
<section id="normalized-temperature-scaled-cross-entropy-loss">
<h3>Normalized Temperature-scaled Cross Entropy Loss<a class="headerlink" href="#normalized-temperature-scaled-cross-entropy-loss" title="Permalink to this heading"></a></h3>
<p><a class="reference external" href="https://arxiv.org/pdf/2002.05709.pdf">A Simple Framework for Contrastive Learning of Visual Representations</a></p>
<p>自监督学习方法，采用数据增强的方法生成正样本对。</p>
<div class="math notranslate nohighlight">
\[\mathcal{L} = \frac{1}{2N} \sum_{k=1}^N \left( \mathcal{L}(2k-1, 2k) + \mathcal{L}(2k, 2k-1) \right)\]</div>
<div class="math notranslate nohighlight">
\[\mathcal{L}(i,j) = - \log \left( \frac{e^{s_{i,j}/\tau}}{\sum_{k=1}^{2N} \mathbf{1}(k \neq i) e^{s_{i,k}/\tau} } \right)\]</div>
<div class="math notranslate nohighlight">
\[s_{i,j} = \frac{z_i^{\top}z_j}{\| z_i \| \| z_j \|}\]</div>
<p><span class="math notranslate nohighlight">\(N\)</span> 是 batch 的大小，<span class="math notranslate nohighlight">\(\tau\)</span> 是温度缩放因子。</p>
</section>
</section>
<section id="id2">
<h2><span class="section-number">12.2. </span>样本对挖掘<a class="headerlink" href="#id2" title="Permalink to this heading"></a></h2>
<section id="packaged-triplets">
<h3>Packaged Triplets<a class="headerlink" href="#packaged-triplets" title="Permalink to this heading"></a></h3>
<p>已经预先采样好的三元组。</p>
</section>
<section id="triplet-margin-miner">
<h3>Triplet-Margin Miner<a class="headerlink" href="#triplet-margin-miner" title="Permalink to this heading"></a></h3>
<p>根据 anchor-positive 与 anchor-negative 距离差来挖掘样本。只选择最困难的样本对是不利于训练的。</p>
<ul class="simple">
<li><p>Hard：<span class="math notranslate nohighlight">\(d_{an} &lt; d_{ap}\)</span></p></li>
<li><p>Semi-Hard：<span class="math notranslate nohighlight">\(d_{an} &gt; d_{ap}\)</span></p></li>
</ul>
</section>
<section id="angular-miner">
<h3>Angular Miner<a class="headerlink" href="#angular-miner" title="Permalink to this heading"></a></h3>
<p>对应 Angular Loss，根据角度阈值构建三元组。</p>
</section>
<section id="batch-hard-miner">
<h3>Batch-Hard Miner<a class="headerlink" href="#batch-hard-miner" title="Permalink to this heading"></a></h3>
<p><a class="reference external" href="https://arxiv.org/pdf/1703.07737.pdf">In Defense of the Triplet Loss for Person Re-Identification</a></p>
<p>在 batch 内选择最困难的正样本对和负样本对。</p>
</section>
<section id="distance-weighted-miner">
<h3>Distance-Weighted Miner<a class="headerlink" href="#distance-weighted-miner" title="Permalink to this heading"></a></h3>
<p><a class="reference external" href="https://arxiv.org/pdf/1706.07567.pdf">Sampling Matters in Deep Embedding Learning</a></p>
<p>根据距离分布均匀采样负样本，可以保证得到的样本分布在一个较大的距离范围，保证样本多样性。</p>
<div class="math notranslate nohighlight">
\[p(d) \propto d^{n-2} \left( 1- \frac{1}{4} d^2 \right) ^{\frac{n-3}{2}}\]</div>
<p><span class="math notranslate nohighlight">\(d\)</span> 表示 anchor 与 negative 的距离，<span class="math notranslate nohighlight">\(n\)</span> 表示 embedding 的维度。</p>
</section>
<section id="hdc-miner">
<h3>HDC Miner<a class="headerlink" href="#hdc-miner" title="Permalink to this heading"></a></h3>
<p><a class="reference external" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Yuan_Hard-Aware_Deeply_Cascaded_ICCV_2017_paper.pdf">Hard-Aware Deeply Cascaded Embedding</a></p>
<p>让更复杂的模型处理更困难的样本。</p>
<p>在 batch 内，对正负样本对都按距离排序，将固定比例的困难样本输入更深层的网络进行提特征、计算损失。</p>
<p>推理阶段，将不同层级的网络输出进行级联。</p>
</section>
<section id="maximum-loss-miner">
<h3>Maximum-Loss Miner<a class="headerlink" href="#maximum-loss-miner" title="Permalink to this heading"></a></h3>
<p>重复多次采样 batch 的子集，提取出损失最大的样本对。</p>
</section>
<section id="multi-similarity-miner">
<h3>Multi-Similarity Miner<a class="headerlink" href="#multi-similarity-miner" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>选择负样本，满足 <span class="math notranslate nohighlight">\(d_{an} &gt; d_{ap}^{max} - \epsilon\)</span></p></li>
<li><p>选择正样本，满足 <span class="math notranslate nohighlight">\(d_{ap} &lt; d_{an}^{min} + \epsilon\)</span></p></li>
</ul>
</section>
<section id="pair-margin-miner">
<h3>Pair-Margin Miner<a class="headerlink" href="#pair-margin-miner" title="Permalink to this heading"></a></h3>
<p>根据距离阈值挑选样本对。</p>
<ul class="simple">
<li><p>选择负样本，满足 <span class="math notranslate nohighlight">\(d_{an} &lt; m_n\)</span></p></li>
<li><p>选择正样本，满足 <span class="math notranslate nohighlight">\(d_{ap} &gt; m_p\)</span></p></li>
</ul>
</section>
</section>
<section id="id4">
<h2><span class="section-number">12.3. </span>参考资料<a class="headerlink" href="#id4" title="Permalink to this heading"></a></h2>
<ol class="arabic simple">
<li><p>A Metric Learning Reality Check</p></li>
</ol>
<blockquote>
<div><p><a class="reference external" href="https://arxiv.org/abs/2003.08505">https://arxiv.org/abs/2003.08505</a></p>
<p><a class="reference external" href="https://github.com/KevinMusgrave/pytorch-metric-learning">https://github.com/KevinMusgrave/pytorch-metric-learning</a></p>
<p><a class="reference external" href="https://kevinmusgrave.github.io/pytorch-metric-learning/">https://kevinmusgrave.github.io/pytorch-metric-learning/</a></p>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>深度度量学习中的损失函数</p></li>
</ol>
<blockquote>
<div><p><a class="reference external" href="https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&amp;mid=2247494208&amp;idx=1&amp;sn=50a940f4ce6093cd6c75f84e6c8efd59&amp;chksm=fbd7582ccca0d13a270878d4aeeda8de15cc4be694b86185a95a74fee4aa9ae90efe87fe1bad&amp;scene=27#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&amp;mid=2247494208&amp;idx=1&amp;sn=50a940f4ce6093cd6c75f84e6c8efd59&amp;chksm=fbd7582ccca0d13a270878d4aeeda8de15cc4be694b86185a95a74fee4aa9ae90efe87fe1bad&amp;scene=27#wechat_redirect</a></p>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p>『深度概念』度量学习中损失函数的学习与深入理解</p></li>
</ol>
<blockquote>
<div><p><a class="reference external" href="https://www.cnblogs.com/xiaosongshine/p/11059762.html">https://www.cnblogs.com/xiaosongshine/p/11059762.html</a></p>
</div></blockquote>
<ol class="arabic simple" start="4">
<li><p>图解SimCLR框架，用对比学习得到一个好的视觉预训练模型</p></li>
</ol>
<blockquote>
<div><p><a class="reference external" href="https://blog.csdn.net/u011984148/article/details/106233313/">https://blog.csdn.net/u011984148/article/details/106233313/</a></p>
</div></blockquote>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="11_nograd.html" class="btn btn-neutral float-left" title="11. pytorch：no_grad()" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../mathematicsAlgorithm/index.html" class="btn btn-neutral float-right" title="数理与算法" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018, fong.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<script type="text/javascript">
    $(document).ready(function() {
     $(".toggle > *").hide();
     $(".toggle .header").show();
     $(".toggle .header").click(function() {
      $(this).parent().children().not(".header").toggle(400);
      $(this).parent().children(".header").toggleClass("open");
     })
    });
</script>


</body>
</html>