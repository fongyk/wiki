

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>12. Deep Metric Learning &mdash; fong alpha documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript">
          var DOCUMENTATION_OPTIONS = {
              URL_ROOT:'../',
              VERSION:'alpha',
              LANGUAGE:'None',
              COLLAPSE_INDEX:false,
              FILE_SUFFIX:'.html',
              HAS_SOURCE:  true,
              SOURCELINK_SUFFIX: '.txt'
          };
      </script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="数理与算法" href="../mathematicsAlgorithm/index.html" />
    <link rel="prev" title="11. pytorch：no_grad()" href="11_nograd.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> fong
          

          
            
            <img src="../_static/logo.jpg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                alpha
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">目录</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../cpp/index.html">C/C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python/index.html">Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../linux/index.html">Linux/Shell</a></li>
<li class="toctree-l1"><a class="reference internal" href="../git/index.html">Git</a></li>
<li class="toctree-l1"><a class="reference internal" href="../machineLearning/index.html">机器学习</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">深度学习</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01_dataParallel.html">1. pytorch：多GPU模式</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_activationFunction.html">2. 激活函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_batchnorm.html">3. Batch Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="04_overfit.html">4. 过拟合</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_modelSave.html">5. pytorch：模型保存与读取</a></li>
<li class="toctree-l2"><a class="reference internal" href="06_cuda.html">6. pytorch: cuda</a></li>
<li class="toctree-l2"><a class="reference internal" href="07_backprop.html">7. 反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="08_optimizer.html">8. 优化算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="09_addModule.html">9. pytorch：Module</a></li>
<li class="toctree-l2"><a class="reference internal" href="10_receptivaField.html">10. 特征图与感受野</a></li>
<li class="toctree-l2"><a class="reference internal" href="11_nograd.html">11. pytorch：no_grad()</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">12. Deep Metric Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#softmax-loss">12.1. Softmax Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#center-loss">12.2. Center Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#large-margin-softmax-loss">12.3. Large Margin Softmax Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#sphereface-loss">12.4. SphereFace Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cosface-loss">12.5. CosFace Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#arcface-loss">12.6. ArcFace Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#contrastive-loss">12.7. Contrastive Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#triplet-loss">12.8. Triplet Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#margin-loss">12.9. Margin Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tuplet-margin-loss">12.10. Tuplet Margin Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#n-pair-loss">12.11. N-pair Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#lifted-structure-loss">12.12. Lifted Structure Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#nca-loss">12.13. NCA Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#proxy-nca-loss">12.14. Proxy NCA Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#proxy-anchor-loss">12.15. Proxy Anchor Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#softtriple-loss">12.16. SoftTriple Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multi-similarity-loss">12.17. Multi-Similarity loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id1">12.18. 参考资料</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mathematicsAlgorithm/index.html">数理与算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../computerNetwork/index.html">计算机网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../link/index.html">资源链接</a></li>
<li class="toctree-l1"><a class="reference internal" href="../softwares/index.html">实用软件</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tech/index.html">Tech</a></li>
<li class="toctree-l1"><a class="reference internal" href="../else/index.html">其他</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">fong</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">深度学习</a> &raquo;</li>
        
      <li>12. Deep Metric Learning</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/deepLearning/12_dml.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="deep-metric-learning">
<h1>12. Deep Metric Learning<a class="headerlink" href="#deep-metric-learning" title="Permalink to this headline">¶</a></h1>
<p>介绍一些 Deep Metric Learning （深度度量学习）的损失函数。</p>
<p>以下损失函数中的 <span class="math notranslate nohighlight">\(x\)</span> 表示 embedding。</p>
<div class="section" id="softmax-loss">
<h2>12.1. Softmax Loss<a class="headerlink" href="#softmax-loss" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_i) = - \log \frac{e^{w^{\top}_{y_i} x_i + b_{y_i}}}{\sum_j e^{w^{\top}_j x_i + b_j}}\]</div>
</div>
<div class="section" id="center-loss">
<h2>12.2. Center Loss<a class="headerlink" href="#center-loss" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://kpzhang93.github.io/papers/eccv2016.pdf">A Discriminative Feature Learning Approach for Deep Face Recognition</a></p>
<p>减小类内差异，每个类别在特征空间分别维护一个类中心。</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathcal{L}(x_i) \ &amp; = &amp;\  \frac{1}{2} \| x_i - c_{y_i} \|_2^2 \\
\Delta c_j \ &amp; = &amp;\  \frac{\sum_{i=1}^{m} \delta(y_i=j) \cdot (c_j - x_i)}{1 + \sum_{i=1}^{m} \delta(y_i=j)}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(m\)</span> 是一个 batch 的大小。</p>
</div>
<div class="section" id="large-margin-softmax-loss">
<h2>12.3. Large Margin Softmax Loss<a class="headerlink" href="#large-margin-softmax-loss" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://arxiv.org/pdf/1612.02295.pdf">Large-Margin Softmax Loss for Convolutional Neural Networks</a></p>
<p>减小类内差异，增大类间差异。</p>
<div class="math notranslate nohighlight">
\[w_j^{\top} x_i = \| w_j \| \| x_i \| \cos(\theta_j)\]</div>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_i) = - \log \left( \frac{e^{\| w_{y_i} \| \| x_i \| \psi(\theta_{y_i}) }}{e^{\| w_{y_i} \| \| x_i \| \psi(\theta_{y_i}) } + \sum_{j \neq y_i} e^{\| w_j \| \| x_i \| \cos(\theta_j) }} \right)\]</div>
<div class="math notranslate nohighlight">
$$
\psi(\theta) =
\begin{cases}
   \cos (m \theta) &amp; &amp; 0 \leq \theta \leq \frac{\pi}{m} \\
   D(\theta) &amp; &amp;  \frac{\pi}{m} &lt; \theta \leq \pi
\end{cases}
$$</div><p><span class="math notranslate nohighlight">\(m\)</span> 表示 margin，<span class="math notranslate nohighlight">\(D(\theta)\)</span> 是一个单调减函数，且 <span class="math notranslate nohighlight">\(D(\frac{\pi}{m})=\cos(\frac{\pi}{m})\)</span> 。</p>
</div>
<div class="section" id="sphereface-loss">
<h2>12.4. SphereFace Loss<a class="headerlink" href="#sphereface-loss" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://arxiv.org/pdf/1704.08063.pdf">SphereFace: Deep Hypersphere Embedding for Face Recognition</a></p>
<p>在 Large Margin Softmax Loss 的基础上，令 <span class="math notranslate nohighlight">\(\| w \| = 1\)</span> 。</p>
<div class="math notranslate nohighlight">
\[w_j^{\top} x_i = \| w_j \| \| x_i \| \cos(\theta_j) = \| x_i \| \cos(\theta_j)\]</div>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_i) = - \log \left( \frac{e^{\| x_i \| \cos(m \theta_{y_i}) }}{e^{\| x_i \| \cos(m \theta_{y_i}) } + \sum_{j \neq y_i} e^{\| x_i \| \cos(\theta_j) }} \right)\]</div>
</div>
<div class="section" id="cosface-loss">
<h2>12.5. CosFace Loss<a class="headerlink" href="#cosface-loss" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://arxiv.org/pdf/1801.09414.pdf">CosFace: Large Margin Cosine Loss for Deep Face Recognition</a></p>
<p>在余弦空间中最大化分类界限。</p>
<div class="math notranslate nohighlight">
\[w_j^{\top} x_i = \| w_j \| \| x_i \| \cos(\theta_j) = \| x_i \| \cos(\theta_j) = s \cos(\theta_j)\]</div>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_i) = - \log \left( \frac{e^{s(\cos(\theta_{y_i}) - m)}}{e^{s(\cos(\theta_{y_i}) - m)} + \sum_{j \neq y_i} e^{s \cos(\theta_j)}} \right)\]</div>
<p><span class="math notranslate nohighlight">\(m\)</span> 表示 margin，<span class="math notranslate nohighlight">\(s\)</span> 表示超球面的半径。</p>
</div>
<div class="section" id="arcface-loss">
<h2>12.6. ArcFace Loss<a class="headerlink" href="#arcface-loss" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://arxiv.org/pdf/1801.07698.pdf">ArcFace: Additive Angular Margin Loss for Deep Face Recognition</a></p>
<p>在角度空间中最大化分类界限。</p>
<div class="math notranslate nohighlight">
\[w_j^{\top} x_i = \| w_j \| \| x_i \| \cos(\theta_j) = \| x_i \| \cos(\theta_j) = s \cos(\theta_j)\]</div>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_i) = - \log \left( \frac{e^{s(\cos(\theta_{y_i} + m))}}{e^{s(\cos(\theta_{y_i} + m))} + \sum_{j \neq y_i} e^{s \cos(\theta_j)}} \right)\]</div>
<p><span class="math notranslate nohighlight">\(m\)</span> 表示 margin，<span class="math notranslate nohighlight">\(s\)</span> 表示超球面的半径。</p>
</div>
<div class="section" id="contrastive-loss">
<h2>12.7. Contrastive Loss<a class="headerlink" href="#contrastive-loss" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf">Dimensionality Reduction by Learning an Invariant Mapping</a></p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_i, x_j) = \mathbf{1} (y_i = y_j) \| x_i - x_j \|_2^2 + \mathbf{1} (y_i \neq y_j) max(0, m - \| x_i - x_j \|_2)^2\]</div>
</div>
<div class="section" id="triplet-loss">
<h2>12.8. Triplet Loss<a class="headerlink" href="#triplet-loss" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf">Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_a, x_p, x_n) = max(0, m + \| x_a - x_p \|_2^2 - \| x_a - x_n \|_2^2)\]</div>
</div>
<div class="section" id="margin-loss">
<h2>12.9. Margin Loss<a class="headerlink" href="#margin-loss" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://arxiv.org/pdf/1706.07567.pdf">Sampling Matters in Deep Embedding Learning</a></p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_i, x_j) = max(0, \alpha + y_{ij} (D_{i,j} - \beta))\]</div>
<p><span class="math notranslate nohighlight">\(y_{ij} \in \{ -1, 1 \}\)</span>，<span class="math notranslate nohighlight">\(D_{ij}\)</span> 表示距离，<span class="math notranslate nohighlight">\(\beta\)</span> 是可学习的参数。</p>
</div>
<div class="section" id="tuplet-margin-loss">
<h2>12.10. Tuplet Margin Loss<a class="headerlink" href="#tuplet-margin-loss" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Deep_Metric_Learning_With_Tuplet_Margin_Loss_ICCV_2019_paper.pdf">Deep Metric Learning with Tuplet Margin Loss</a></p>
<p>每个 batch 包含 <span class="math notranslate nohighlight">\(k\)</span> 个类别，每个类别 <span class="math notranslate nohighlight">\(n\)</span> 个样本，从其他的 <span class="math notranslate nohighlight">\(k-1\)</span> 个类别中随机选取一个样本作为负例，可以组成 <span class="math notranslate nohighlight">\(kn(n-1)\)</span> 个三元组。</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_a, x_p) = \log \left( 1 + \sum_{i=1}^{k-1} e^{s \left( \cos(\theta_{an_i}) - \cos(\theta_{ap} - \beta) \right)} \right)\]</div>
<p><span class="math notranslate nohighlight">\(s\)</span> 是一个缩放因子。</p>
</div>
<div class="section" id="n-pair-loss">
<h2>12.11. N-pair Loss<a class="headerlink" href="#n-pair-loss" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="http://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf">Improved Deep Metric Learning with Multi-class N-pair Loss Objective</a></p>
<p>利用一个 batch 内的所有负例。</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_i, x_i^+) = \log(1 + \sum_{j \neq i} e^{x_i^{\top} x_j^+ - x_i^{\top} x_i^+})\]</div>
</div>
<div class="section" id="lifted-structure-loss">
<h2>12.12. Lifted Structure Loss<a class="headerlink" href="#lifted-structure-loss" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://arxiv.org/pdf/1511.06452.pdf">Deep Metric Learning via Lifted Structured Feature Embedding</a></p>
<p>利用一个 batch 内的所有正负样本对。</p>
<div class="math notranslate nohighlight">
\[\mathcal{L} = \frac{1}{2 | \mathcal{P} |} \sum_{(i,j) \in \mathcal{P}} max(0, \mathcal{L}_{i,j})^2\]</div>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{i,j} = max \left( \underset{(i,k) \in \mathcal{N}}{max}(\alpha - D_{i,k}), \underset{(j,l) \in \mathcal{N}}{max}(\alpha - D_{j,l}) \right) + D_{i,j}\]</div>
<p><span class="math notranslate nohighlight">\(\mathcal{P}\)</span> 表示正样本对，<span class="math notranslate nohighlight">\(\mathcal{N}\)</span> 表示负样本对，<span class="math notranslate nohighlight">\(D_{i,j}\)</span> 表示样本对的距离，<span class="math notranslate nohighlight">\(\alpha\)</span> 表示 margin。</p>
</div>
<div class="section" id="nca-loss">
<h2>12.13. NCA Loss<a class="headerlink" href="#nca-loss" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://www.cs.toronto.edu/~hinton/absps/nca.pdf">Neighbourhood Components Analysis</a></p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x, y, \mathcal{Z}) = - \log \left( \frac{e^{-d(x, y)}}{\sum_{z \in \mathcal{Z}} e^{-d(x,z)}} \right)\]</div>
<p><span class="math notranslate nohighlight">\(d\)</span> 是距离函数，<span class="math notranslate nohighlight">\(y\)</span> 是正例，<span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> 是负例集合。</p>
</div>
<div class="section" id="proxy-nca-loss">
<h2>12.14. Proxy NCA Loss<a class="headerlink" href="#proxy-nca-loss" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://arxiv.org/pdf/1703.07464.pdf">No Fuss Distance Metric Learning using Proxies</a></p>
<p>每一个类别都有一个可学习的 proxy，用来近似真实的数据点。<span class="math notranslate nohighlight">\(x\)</span> 对应的正例为本类别的 proxy <span class="math notranslate nohighlight">\(p(y)\)</span>，负例为所有其他类别的 proxy <span class="math notranslate nohighlight">\(p(\mathcal{Z})\)</span> 。</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x) = - \log \left( \frac{e^{-d(x, p(y))}}{\sum_{p(z) \in p(\mathcal{Z})} e^{-d(x,p(z))}} \right)\]</div>
</div>
<div class="section" id="proxy-anchor-loss">
<h2>12.15. Proxy Anchor Loss<a class="headerlink" href="#proxy-anchor-loss" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://arxiv.org/pdf/2003.13911.pdf">Proxy Anchor Loss for Deep Metric Learning</a></p>
<p>为每一个类别赋予了一个 proxy，将一个 batch 的样本和所有的 proxy 之间求距离，拉近每个类别的样本和该类别对应的 proxy 之间的距离，拉远与其他类别的 proxy 之间的距离。相比于 Proxy NCA Loss，更加充分地利用了 batch 的数据。</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\mathcal{X}) = \frac{1}{| \mathcal{P}^+ |} \sum_{p \in \mathcal{P}^+} \log \left( 1 + \sum_{x \in \mathcal{X}_p^+} e^{-\alpha (s(x,p) - \delta)} \right) + \frac{1}{| \mathcal{P} |} \sum_{p \in \mathcal{P}} \log \left( 1 + \sum_{x \in \mathcal{X}_p^-} e^{\alpha (s(x,p) + \delta)} \right)\]</div>
<p><span class="math notranslate nohighlight">\(\mathcal{X}\)</span> 表示一个 batch 内所有样本的 embedding 集合；<span class="math notranslate nohighlight">\(\mathcal{P}^+\)</span> 表示正例 proxy 的集合，也就是 batch 内的样本对应的 proxy 的集合；<span class="math notranslate nohighlight">\(\mathcal{P}\)</span> 表示所有 proxy 的集合，也就是所有类别对应的 proxy 的集合；<span class="math notranslate nohighlight">\(\mathcal{X}_p^+\)</span> 表示与 <span class="math notranslate nohighlight">\(p\)</span> 同一类别的 embedding 集合，<span class="math notranslate nohighlight">\(\mathcal{X}_p^- = \mathcal{X} - \mathcal{X}_p^+\)</span> ；<span class="math notranslate nohighlight">\(s\)</span> 表示余弦相似度。</p>
</div>
<div class="section" id="softtriple-loss">
<h2>12.16. SoftTriple Loss<a class="headerlink" href="#softtriple-loss" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Qian_SoftTriple_Loss_Deep_Metric_Learning_Without_Triplet_Sampling_ICCV_2019_paper.pdf">SoftTriple Loss: Deep Metric Learning Without Triplet Sampling</a></p>
<p>考虑到同类数据的多样性，为每类数据学习 <span class="math notranslate nohighlight">\(K\)</span> 个类中心；通过正则项自适应地合并相似的类中心。</p>
<div class="math notranslate nohighlight">
\[S_{i, c} = \sum_{k=1}^K \frac{e^{\frac{1}{\gamma}x_i^{\top}w_c^k}}{\sum_{t=1}^K e^{\frac{1}{\gamma}x_i^{\top}w_c^t}} x_i^{\top}w_c^k\]</div>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_i) = - \log \frac{e^{\lambda (S_{i, y_i} - \delta )}}{e^{\lambda (S_{i, y_i} - \delta )} + \sum_{j \neq y_i} e^{\lambda S_{i, j}}}\]</div>
</div>
<div class="section" id="multi-similarity-loss">
<h2>12.17. Multi-Similarity loss<a class="headerlink" href="#multi-similarity-loss" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Multi-Similarity_Loss_With_General_Pair_Weighting_for_Deep_Metric_Learning_CVPR_2019_paper.pdf">Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning</a></p>
<p>为给每一个样本对动态赋予一个权重，这个权重是体现在梯度上的。给样本赋权的核心在于判断样本的局部分布，即它们之间的相似性。局部样本之间的分布和相互关系并不仅仅取决于当前两个样本之间的距离或相似性，还取决于当前样本对与其周围样本对之间的关系。</p>
<div class="math notranslate nohighlight">
\[w_{ij}^- = \frac{1}{e^{\beta(\lambda - S_{ij})} + \sum_{k \in \mathcal{N}_i} e^{\beta(S_{ik} - S_{ij})}}\]</div>
<div class="math notranslate nohighlight">
\[w_{ij}^+ = \frac{1}{e^{-\alpha(\lambda - S_{ij})} + \sum_{k \in \mathcal{P}_i} e^{-\alpha(S_{ik} - S_{ij})}}\]</div>
<div class="math notranslate nohighlight">
\[\mathcal{L}(x_i) = \frac{1}{\alpha} \log \left( 1 + \sum_{k \in \mathcal{P}_i} e^{-\alpha (S_{ik} - \lambda)} \right) + \frac{1}{\beta} \log \left( 1 + \sum_{k \in \mathcal{N}_i} e^{\beta (S_{ik} - \lambda)} \right)\]</div>
<p><span class="math notranslate nohighlight">\(\mathcal{P}_i\)</span> 表示正样本集合，<span class="math notranslate nohighlight">\(\mathcal{N}_i\)</span> 表示负样本集合，<span class="math notranslate nohighlight">\(S_{ij}\)</span> 表示样本对的相似度。</p>
</div>
<div class="section" id="id1">
<h2>12.18. 参考资料<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li>A Metric Learning Reality Check</li>
</ol>
<blockquote>
<div><p><a class="reference external" href="https://arxiv.org/abs/2003.08505">https://arxiv.org/abs/2003.08505</a></p>
<p><a class="reference external" href="https://github.com/KevinMusgrave/pytorch-metric-learning">https://github.com/KevinMusgrave/pytorch-metric-learning</a></p>
<p><a class="reference external" href="https://kevinmusgrave.github.io/pytorch-metric-learning/">https://kevinmusgrave.github.io/pytorch-metric-learning/</a></p>
</div></blockquote>
<ol class="arabic simple" start="2">
<li>深度度量学习中的损失函数</li>
</ol>
<blockquote>
<div><a class="reference external" href="https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&amp;mid=2247494208&amp;idx=1&amp;sn=50a940f4ce6093cd6c75f84e6c8efd59&amp;chksm=fbd7582ccca0d13a270878d4aeeda8de15cc4be694b86185a95a74fee4aa9ae90efe87fe1bad&amp;scene=27#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&amp;mid=2247494208&amp;idx=1&amp;sn=50a940f4ce6093cd6c75f84e6c8efd59&amp;chksm=fbd7582ccca0d13a270878d4aeeda8de15cc4be694b86185a95a74fee4aa9ae90efe87fe1bad&amp;scene=27#wechat_redirect</a></div></blockquote>
<ol class="arabic simple" start="3">
<li>『深度概念』度量学习中损失函数的学习与深入理解</li>
</ol>
<blockquote>
<div><a class="reference external" href="https://www.cnblogs.com/xiaosongshine/p/11059762.html">https://www.cnblogs.com/xiaosongshine/p/11059762.html</a></div></blockquote>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../mathematicsAlgorithm/index.html" class="btn btn-neutral float-right" title="数理与算法" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="11_nograd.html" class="btn btn-neutral float-left" title="11. pytorch：no_grad()" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, fong

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
  
<script type="text/javascript">
    $(document).ready(function() {
     $(".toggle > *").hide();
     $(".toggle .header").show();
     $(".toggle .header").click(function() {
      $(this).parent().children().not(".header").toggle(400);
      $(this).parent().children(".header").toggleClass("open");
     })
    });
</script>


</body>
</html>