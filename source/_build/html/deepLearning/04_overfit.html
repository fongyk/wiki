

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>4. 过拟合 &mdash; fong alpha documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript">
          var DOCUMENTATION_OPTIONS = {
              URL_ROOT:'../',
              VERSION:'alpha',
              LANGUAGE:'None',
              COLLAPSE_INDEX:false,
              FILE_SUFFIX:'.html',
              HAS_SOURCE:  true,
              SOURCELINK_SUFFIX: '.txt'
          };
      </script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5. pytorch：模型保存与读取" href="05_modelSave.html" />
    <link rel="prev" title="3. Batch Normalization" href="03_batchnorm.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> fong
          

          
            
            <img src="../_static/logo.jpg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                alpha
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">目录</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../cpp/index.html">C/C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python/index.html">Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../linux/index.html">Linux/Shell</a></li>
<li class="toctree-l1"><a class="reference internal" href="../git/index.html">Git</a></li>
<li class="toctree-l1"><a class="reference internal" href="../machineLearning/index.html">机器学习</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">深度学习</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01_dataParallel.html">1. pytorch：多GPU模式</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_activationFunction.html">2. 激活函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_batchnorm.html">3. Batch Normalization</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">4. 过拟合</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">4.1. 表现</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">4.2. 原因</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id4">4.3. 解决方案</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dropout-numpy">4.4. dropout 的 numpy 实现</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id5">4.5. 附：正则化</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id6">4.6. 参考资料</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="05_modelSave.html">5. pytorch：模型保存与读取</a></li>
<li class="toctree-l2"><a class="reference internal" href="06_cuda.html">6. pytorch: cuda()</a></li>
<li class="toctree-l2"><a class="reference internal" href="07_backprop.html">7. 反向传播</a></li>
<li class="toctree-l2"><a class="reference internal" href="08_optimizer.html">8. 优化算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="09_addModule.html">9. pytorch：add_module</a></li>
<li class="toctree-l2"><a class="reference internal" href="10_receptivaField.html">10. 特征图与感受野</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mathematicsAlgorithm/index.html">数理与算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../computerNetwork/index.html">计算机网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="../link/index.html">资源链接</a></li>
<li class="toctree-l1"><a class="reference internal" href="../softwares/index.html">实用软件</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tech/index.html">Tech</a></li>
<li class="toctree-l1"><a class="reference internal" href="../else/index.html">其他</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">fong</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">深度学习</a> &raquo;</li>
        
      <li>4. 过拟合</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/deepLearning/04_overfit.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="id1">
<h1>4. 过拟合<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<p>复杂的模型将训练数据的抽样误差考虑在内，对抽样误差也进行了拟合。过拟合的模型可以看成是完全记忆型模型。</p>
<div class="section" id="id2">
<h2>4.1. 表现<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>训练误差小，测试误差大，泛化能力差。</p>
</div>
<div class="section" id="id3">
<h2>4.2. 原因<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>训练集大小与模型复杂度不匹配；</li>
<li>样本的噪声太大甚至掩盖了真实样本的分布规律；</li>
<li>训练迭代次数太多（over-training）。</li>
</ul>
</div>
<div class="section" id="id4">
<h2>4.3. 解决方案<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<p><strong>1</strong>. 调小模型复杂度。</p>
<p><strong>2</strong>. data augmentation.</p>
<p><strong>3</strong>. dropout. dropout 随机屏蔽了部分神经元的前向和反向传播，有利于保持神经元的独立性；由于每次迭代都屏蔽不同神经元，因此模型的训练过程可以看作是多个模型的集成。</p>
<p><strong>4</strong>. early stopping. 记录观察validation accuracy，及时停止训练。</p>
<p><strong>5</strong>. 集成学习。Bagging：并行化模型生成，减小模型variance。Boosting：串行化模型生成，减小模型bias。</p>
<p><strong>6</strong>. 正则化。</p>
<blockquote>
<div><ul class="simple">
<li>L0正则化（非0元素个数），难以优化求解（NP-Hard）。</li>
<li>L1正则化（元素绝对值之和， Lasso regression），是L0范数的最优凸近似，使权值稀疏。权值稀疏的好处：特征选择 &amp;&amp; 可解释性。</li>
<li>L2正则化（元素平方和，Ridge regression / weight dacay），使权值分布均匀且值较小。</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="dropout-numpy">
<h2>4.4. dropout 的 numpy 实现<a class="headerlink" href="#dropout-numpy" title="Permalink to this headline">¶</a></h2>
<p>前向传播：生成 mask，乘以当前层的激活函数输出。新的输出需要除以 keep_prob，保证能量一致。</p>
<p>反向传播：也要关闭 mask 对应的神经元，同样也需要除以 keep_prob。</p>
<div class="toggle docutils container">
<div class="header docutils container">
<span class="math notranslate nohighlight">\(\color{darkgreen}{Code}\)</span></div>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward_propagation_with_dropout</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">keep_prob</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">):</span>

<span class="sd">&quot;&quot;&quot;</span>

<span class="sd">Implements the forward propagation: LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; SIGMOID.</span>

<span class="sd">Arguments:</span>

<span class="sd">X -- input dataset, of shape (2, number of examples)</span>

<span class="sd">parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, &quot;W2&quot;, &quot;b2&quot;:</span>

<span class="sd">                W1 -- weight matrix of shape (20, 2)</span>

<span class="sd">                b1 -- bias vector of shape (20, 1)</span>

<span class="sd">                W2 -- weight matrix of shape (1, 20)</span>

<span class="sd">                b2 -- bias vector of shape (1, 1)</span>

<span class="sd">keep_prob - probability of keeping a neuron active during drop-out, scalar</span>

<span class="sd">Returns:</span>

<span class="sd">A2 -- last activation value, output of the forward propagation, of shape (1,1)</span>

<span class="sd">cache -- tuple, information stored for computing the backward propagation</span>

<span class="sd">&quot;&quot;&quot;</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># retrieve parameters</span>

<span class="n">W1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W1&quot;</span><span class="p">]</span>

<span class="n">b1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b1&quot;</span><span class="p">]</span>

<span class="n">W2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W2&quot;</span><span class="p">]</span>

<span class="n">b2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b2&quot;</span><span class="p">]</span>


<span class="c1"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span>
<span class="c1"># Z1 = W1 x X + b1, A1 = relu(Z1), A1 = dropout(A1)</span>
<span class="c1"># Z2 = W2 x A1 + b2, A2 = sigmoid(Z2)</span>

<span class="n">Z1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>

<span class="n">A1</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">Z1</span><span class="p">)</span>

<span class="c1"># 4 steps</span>

<span class="n">D1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">Z1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Z1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>     <span class="c1"># Step 1: initialize matrix D1 = np.random.rand(..., ...)</span>

<span class="n">D1</span> <span class="o">=</span> <span class="n">D1</span> <span class="o">&lt;</span> <span class="n">keep_prob</span>                               <span class="c1"># Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)</span>

<span class="n">A1</span> <span class="o">=</span> <span class="n">A1</span> <span class="o">*</span> <span class="n">D1</span>                                        <span class="c1"># Step 3: shut down some neurons of A1</span>

<span class="n">A1</span> <span class="o">=</span> <span class="n">A1</span> <span class="o">/</span> <span class="n">keep_prob</span>                                 <span class="c1"># Step 4: scale the value of neurons that haven&#39;t been shut down</span>

<span class="n">Z2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">A1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>

<span class="n">A2</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Z2</span><span class="p">)</span>

<span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">Z1</span><span class="p">,</span> <span class="n">D1</span><span class="p">,</span> <span class="n">A1</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">Z2</span><span class="p">,</span> <span class="n">D2</span><span class="p">,</span> <span class="n">A2</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>

<span class="k">return</span> <span class="n">A3</span><span class="p">,</span> <span class="n">cache</span>
</pre></div>
</td></tr></table></div>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">backward_propagation_with_dropout</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">):</span>

<span class="sd">&quot;&quot;&quot;</span>

<span class="sd">Implements the backward propagation of our baseline model to which we added dropout.</span>

<span class="sd">Arguments:</span>

<span class="sd">X -- input dataset, of shape (2, number of examples)</span>

<span class="sd">Y -- &quot;true&quot; labels vector, of shape (output size, number of examples)</span>

<span class="sd">cache -- cache output from forward_propagation_with_dropout()</span>

<span class="sd">keep_prob - probability of keeping a neuron active during drop-out, scalar</span>


<span class="sd">Returns:</span>

<span class="sd">gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span>

<span class="sd">&quot;&quot;&quot;</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="p">(</span><span class="n">Z1</span><span class="p">,</span> <span class="n">D1</span><span class="p">,</span> <span class="n">A1</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">Z2</span><span class="p">,</span> <span class="n">D2</span><span class="p">,</span> <span class="n">A2</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span> <span class="o">=</span> <span class="n">cache</span>


<span class="n">dZ2</span> <span class="o">=</span> <span class="n">A2</span> <span class="o">-</span> <span class="n">Y</span> <span class="c1"># logistic regression</span>

<span class="n">dW2</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ2</span><span class="p">,</span> <span class="n">A1</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>  <span class="c1"># logistic regression</span>

<span class="n">db2</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>


<span class="n">dA1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dZ2</span><span class="p">)</span>

<span class="n">dA1</span> <span class="o">=</span> <span class="n">D1</span> <span class="o">*</span> <span class="n">dA1</span>                     <span class="c1"># Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation</span>

<span class="n">dA1</span> <span class="o">=</span> <span class="n">dA1</span> <span class="o">/</span> <span class="n">keep_prob</span>              <span class="c1"># Step 2: Scale the value of neurons that haven&#39;t been shut down</span>

<span class="n">dZ1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">dA1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="n">A1</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">))</span>   <span class="c1"># Hadamard product, i.e., element-wise product</span>

<span class="n">dW1</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

<span class="n">db1</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>


<span class="n">gradients</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;dA2&quot;</span><span class="p">:</span> <span class="n">dA2</span><span class="p">,</span> <span class="s2">&quot;dZ2&quot;</span><span class="p">:</span> <span class="n">dZ2</span><span class="p">,</span> <span class="s2">&quot;dW2&quot;</span><span class="p">:</span> <span class="n">dW2</span><span class="p">,</span> <span class="s2">&quot;db2&quot;</span><span class="p">:</span> <span class="n">db2</span><span class="p">,</span>
            <span class="s2">&quot;dA1&quot;</span><span class="p">:</span> <span class="n">dA1</span><span class="p">,</span> <span class="s2">&quot;dZ1&quot;</span><span class="p">:</span> <span class="n">dZ1</span><span class="p">,</span> <span class="s2">&quot;dW1&quot;</span><span class="p">:</span> <span class="n">dW1</span><span class="p">,</span> <span class="s2">&quot;db1&quot;</span><span class="p">:</span> <span class="n">db1</span>
            <span class="p">}</span>

<span class="k">return</span> <span class="n">gradients</span>
</pre></div>
</td></tr></table></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="id5">
<h2>4.5. 附：正则化<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[L_q\text{-norm}: \ \| w \|^q_q = \sum_j | w_j |^q.\]</div>
<a class="reference internal image-reference" href="../_images/04_norm.jpg"><img alt="../_images/04_norm.jpg" class="align-center" src="../_images/04_norm.jpg" style="width: 600px;" /></a>
<a class="reference internal image-reference" href="../_images/04_norm2.jpg"><img alt="../_images/04_norm2.jpg" class="align-center" src="../_images/04_norm2.jpg" style="width: 400px;" /></a>
<p>我们通常只对权重（weight）做正则惩罚，而不对偏置（bias）做正则惩罚。精确拟合偏置所需的数据通常比拟合权重少得多。每个权重会指定两个变量（前层-后层）
如何相互作用，我们需要在各种条件下观察这两个变量才能良好地拟合权重；而每个偏置仅控制一个单变量（后层），这意味着：即使不对其进行正则化也不会导致太大的方差。
另外，对偏置进行正则化可能导致明显的欠拟合。</p>
<dl class="docutils">
<dt>No Free Lunch Theorem</dt>
<dd>没有一个机器学习算法总是比其他算法好。这意味着机器学习研究不是要找一个通用的学习算法或是最好的学习算法，而是理解
什么样的分布与人工智能获取的经验分布相关，以及什么样的学习算法在我们关注的数据分布上效果更好。</dd>
<dt>Occam’s Razor</dt>
<dd>如果关于同一个问题有许多种理论，每一种都能作出同样准确的预言，那么应该挑选其中使用假定最少的。尽管越复杂的方法通常能做出越好的预言，但是在不考虑预言能力（即结果大致相同）的情况下，假设越少越好。</dd>
</dl>
</div>
<div class="section" id="id6">
<h2>4.6. 参考资料<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li>深度学习（Deep Learning）基础概念8：L2正则化（L2 Regularization）、Dropout原理及其python实现</li>
</ol>
<blockquote>
<div><a class="reference external" href="https://zhuanlan.zhihu.com/p/29592806">https://zhuanlan.zhihu.com/p/29592806</a></div></blockquote>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="05_modelSave.html" class="btn btn-neutral float-right" title="5. pytorch：模型保存与读取" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="03_batchnorm.html" class="btn btn-neutral float-left" title="3. Batch Normalization" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, fong

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
  
<script type="text/javascript">
    $(document).ready(function() {
     $(".toggle > *").hide();
     $(".toggle .header").show();
     $(".toggle .header").click(function() {
      $(this).parent().children().not(".header").toggle(400);
      $(this).parent().children(".header").toggleClass("open");
     })
    });
</script>


</body>
</html>