Logistic Regression
============================

模型：

.. math::

  h_\theta(\mathbf{x}) & = & \  g(\theta^\top \mathbf{x}),\\
  g(z) & = & \  \frac{1}{1+e^{-z}},\\
  g^\prime(z) & = & \  (1-g(z))g(z) \in (0, 0.25].

对数损失函数（极大似然）：

.. math::

  \mathcal{l}_\theta = -\frac{1}{m} \sum_{i=1}^m y^{(i)} \log h_\theta(\mathbf{x}^{(i)}) + (1 - y^{(i)}) \log(1 - h_\theta(\mathbf{x}^{(i)}))

虽然使用了sigmoid函数，但该模型仍然是线性分类器，因为即使不经过sigmoid函数也可以得出分类结果（与0比较），sigmoid将其转化为概率。

基本假设
-----------

1. 数据服从伯努利分布 :math:`y \sim Bernoulli(\phi)`

2. 样例为正例的概率为 :math:`\phi=h_\theta(\mathbf{x})`

求解方法
------------

梯度下降
  - 批梯度下降：全局最优；每次参数更新需要遍历所有样本，计算量大，效率低。

  - 随机梯度下降（SGD）：以高方差频繁更新，能跳到新的、更好的局部最优解；收敛到局部最优的过程更加复杂。

  - 小批量梯度下降：减少了参数更新次数，达到更稳定的收敛结果。

优缺点
-------

优点
  - 模型简单，可解释性好，效果不错

  - 训练速度快，资源占用少

  - 直接输出样本的分类概率，便于做阈值划分

缺点
  - 准确性不高

  - 很难处理数据不平衡问题

  - 只能处理线性问题

  - 逻辑回归本身无法筛选特征

解析
------------

1. 为什么使用极大似然函数作为损失函数？

  - 极大似然：希望最大化每个样本的分类正确概率，样本服从伯努利分布。

  - 将极大似然取对数后就等同于对数损失函数，在LR模型中，这个损失函数使参数更新速度较快：

    .. math::

      \theta_j \leftarrow \theta_j + \alpha \times \frac{1}{m} \sum_{i=1}^m (y^{(i)} - h_\theta(\mathbf{x}^{(i)}))\mathbf{x}_j^{(i)}

    只与 :math:`y^{(i)},\mathbf{x}^{(i)}` 有关，与 :math:`h_\theta` 的梯度无关。

  - 为什么不用平方损失函数（多用于线性回归）？在线性回归中，前提假设是 :math:`y` 服从正态分布，即 :math:`y \sim \mathcal{N}(\mu, \sigma^2)` 。
    另外，如果使用平方损失函数， :math:`\theta` 更新与 :math:`h_\theta` 的梯度有关，而sigmoid函数的梯度在定义域内小于0.25，导致参数更新慢。

2. 训练中如何有很多特征高度相关或将某个特征重复100遍，影响如何？

  如果损失函数收敛，不影响分类结果（每个特征对应的权重 :math:`\theta_j` 变为原来的百分之一）。将相关特征去除，使模型具有更好的解释性，也能加快训练速度。

参考资料
------------

1. 逻辑回归的常见面试点总结

  http://www.cnblogs.com/ModifyRong/p/7739955.html

2. LR逻辑斯回归分析（优缺点）

  https://blog.csdn.net/touch_dream/article/details/79371462

3. logistic 回归（内附推导）

  https://www.jianshu.com/p/894bda167422

4. 周志华《机器学习》Page 57 -- 60。
