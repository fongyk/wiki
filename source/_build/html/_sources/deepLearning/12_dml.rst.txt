Deep Metric Learning
=========================

介绍一些 Deep Metric Learning （深度度量学习）的损失函数。

以下损失函数中的 :math:`x` 表示 embedding。

Softmax Loss
--------------------------

.. math::

    \mathcal{L}(x_i) = - \log \frac{e^{w^{\top}_{y_i} x_i + b_{y_i}}}{\sum_j e^{w^{\top}_j x_i + b_j}}


Center Loss
--------------------------

`A Discriminative Feature Learning Approach for Deep Face Recognition <https://kpzhang93.github.io/papers/eccv2016.pdf>`_

减小类内差异，每个类别在特征空间分别维护一个类中心。


.. math::

    \mathcal{L}(x_i) &=& \frac{1}{2} \| x_i - c_{y_i} \|_2^2 \\
    \Delta c_j &=& \frac{\sum_{i=1}{m} \delta(y_i=j) \cdot (c_j - x_i)}{1 + \sum_{i=1}{m} \delta(y_i=j)}

:math:`m` 是一个 batch 的大小。


Large Margin Softmax Loss
-----------------------------

`Large-Margin Softmax Loss for Convolutional Neural Networks <https://arxiv.org/pdf/1612.02295.pdf>`_

减小类内差异，增大类间差异。

.. math::

    \mathcal{L}(x_i) = - \log \left( \frac{e^{\| w_{y_i} \| \| x_i \| \psi(\theta_{y_i}) }}{e^{\| w_{y_i} \| \| x_i \| \psi(\theta_{y_i}) } + \sum_{j \neq y_i} e^{\| w_j \| \| x_i \| \cos(\theta_j) }} \right)

.. math::
  :nowrap:

  $$
  \psi(\theta) =
  \begin{cases}
     \cos (m \theta) & & 0 \leq \theta \leq \frac{\pi}{m} \\
     D(\theta) & &  \frac{\pi}{m} < \theta \leq \pi
  \end{cases}
  $$

:math:`m` 表示 margin，:math:`D(\theta)` 是一个单调减函数，且 :math:`D(\frac{\pi}{m})=\cos(\frac{\pi}{m})` 。

SphereFace Loss
---------------------

`SphereFace: Deep Hypersphere Embedding for Face Recognition <https://arxiv.org/pdf/1704.08063.pdf>`_

在 Large Margin Softmax Loss 的基础上，令 :math:`\| w \| = 1` 。

.. math::

    \mathcal{L}(x_i) = - \log \left( \frac{e^{\| x_i \| \cos(m \theta_{y_i}) }}{e^{\| x_i \| \cos(m \theta_{y_i}) } + \sum_{j \neq y_i} e^{\| x_i \| \cos(\theta_j) }} \right)


CosFace Loss
------------------

`CosFace: Large Margin Cosine Loss for Deep Face Recognition <https://arxiv.org/pdf/1801.09414.pdf>`_

在余弦空间中最大化分类界限。

.. math::

    \mathcal{L}(x_i) = - \log \left( \frac{s(\cos(\theta_{y_i}) - m)}{s(\cos(\theta_{y_i}) - m) + \sum_{j \neq y_i} s \cos(\theta_j)} \right)

:math:`m` 表示 margin，:math:`s` 表示超球面的半径。


ArcFace Loss
------------------

`ArcFace: Additive Angular Margin Loss for Deep Face Recognition <https://arxiv.org/pdf/1801.07698.pdf>`_

在角度空间中最大化分类界限。

.. math::

    \mathcal{L}(x_i) = - \log \left( \frac{s(\cos(\theta_{y_i} + m))}{s(\cos(\theta_{y_i} + m)) + \sum_{j \neq y_i} s \cos(\theta_j)} \right)

:math:`m` 表示 margin，:math:`s` 表示超球面的半径。


Contrastive Loss
--------------------------

`Dimensionality Reduction by Learning an Invariant Mapping <http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf>`_

.. math::

    \mathcal{L}(x_i, x_j) = \mathbf{1} (y_i = y_j) \| x_i - x_j \|_2^2 + \mathbf{1} (y_i \neq y_j) max(0, m - \| x_i - x_j \|_2)^2


Triplet Loss
--------------------------

`Distance Metric Learning for Large Margin Nearest Neighbor Classification <https://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf>`_

.. math::

    \mathcal{L}(x_a, x_p, x_n) = max(0, m + \| x_a - x_p \|_2^2 - \| x_a - x_n \|_2^2)

Margin Loss
--------------------------

`Sampling Matters in Deep Embedding Learning <https://arxiv.org/pdf/1706.07567.pdf>`_

.. math::

    \mathcal{L}(x_i, x_j) = max(0, \alpha + y_{ij} (D_{i,j} - \beta))

:math:`y_{ij} \in \{ -1, 1 \}`，:math:`D_{ij}` 表示距离，:math:`\beta` 是可学习的参数。


Tuplet Margin Loss
--------------------------

`Deep Metric Learning with Tuplet Margin Loss <http://openaccess.thecvf.com/content_ICCV_2019/papers/Yu_Deep_Metric_Learning_With_Tuplet_Margin_Loss_ICCV_2019_paper.pdf>`_

.. math::

    \mathcal{L} = \log \left( 1 + \sum_{i=1}^{k-1} e^{s \left( \cos(\theta_{an_i}) - \cos(\theta_{ap} - \beta) \right)} \right)

:math:`s` 是一个缩放因子。 

N-pair Loss
--------------------------

`Improved Deep Metric Learning with Multi-class N-pair Loss Objective <http://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf>`_

利用一个 batch 内的所有负例。

.. math::

    \mathcal{L}(x_i, x_i^+) = log(1 + \sum_{j \neq i} e^{x_i^{\top} x_j^+ - x_i^{\top} x_i^+})


Lifted Structure Loss
-----------------------------

`Deep Metric Learning via Lifted Structured Feature Embedding <https://arxiv.org/pdf/1511.06452.pdf>`_

利用一个 batch 内的所有正负样本对。

.. math::

    \mathcal{L} &=& \frac{1}{2|\mathcal{P}|} \sum_{(i,j) \in \mathcal{P}} max(0, \mathcal{L}_{i,j})^2
    \mathcal{L}(i, j) &=& max(\underset{(i,k) \in \mathcal{N}}{max}(\alpha - D_{i,k}), \underset{(j,l) \in \mathcal{N}}{max}(\alpha - D_{j,l})) + D_{i,j}

:math:`\mathcal{P}` 表示正样本对，:math:`\mathcal{N}` 表示负样本对，:math:`D_{i,j}` 表示样本对的距离，:math:`\alpha` 表示 margin。

NCA Loss
--------------------------

` <>`_

假设 :math:`W` 代表着训练集中的一小部分数据，在采样时通过选择与 :math:`W` 中距离最近的样本作为代理（proxy）。

.. math::

    p(u) &=& \underset{w \in W}{argmin} d(u, w) \\
    \mathcal{L}

Proxy NCA Loss
--------------------------

`No Fuss Distance Metric Learning using Proxies <https://arxiv.org/pdf/1703.07464.pdf>`_

假设 :math:`W` 代表着训练集中的一小部分数据，在采样时通过选择与 :math:`W` 中距离最近的样本作为代理（proxy）。

.. math::

    p(u) &=& \underset{w \in W}{argmin} d(u, w) \\
    \mathcal{L}

Proxy Anchor Loss
--------------------------

` <>`_



SoftTriple Loss
----------------------

` <>`_


Multi-Similarity loss
---------------------------

` <>`_


参考资料
-------------

1. A Metric Learning Reality Check

  https://arxiv.org/abs/2003.08505

  https://github.com/KevinMusgrave/pytorch-metric-learning

  https://kevinmusgrave.github.io/pytorch-metric-learning/

2. 深度度量学习中的损失函数

  https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247494208&idx=1&sn=50a940f4ce6093cd6c75f84e6c8efd59&chksm=fbd7582ccca0d13a270878d4aeeda8de15cc4be694b86185a95a74fee4aa9ae90efe87fe1bad&scene=27#wechat_redirect

3. 『深度概念』度量学习中损失函数的学习与深入理解

  https://www.cnblogs.com/xiaosongshine/p/11059762.html